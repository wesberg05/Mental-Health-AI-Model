{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3djEsXLU71wyAEI/QyukW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bc040ad0865b485e96b2f737ec9de857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a32420b6eaab418c901de07c3973794e",
              "IPY_MODEL_212316f1377f408d9cded47c9882a91c",
              "IPY_MODEL_2f1800be19a04f4d98e2b2f57ff0071e"
            ],
            "layout": "IPY_MODEL_737bc35bae3343239ab2af372ac0b414"
          }
        },
        "a32420b6eaab418c901de07c3973794e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ae0e304507472cb701570c4091f9f3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_69109c725cbf495e92b2c81b37614921",
            "value": "Map:â€‡100%"
          }
        },
        "212316f1377f408d9cded47c9882a91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef669941fd294fadafc55f978b25fc38",
            "max": 352,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8676bbb977f44078ac1a9fd9f8b9850c",
            "value": 352
          }
        },
        "2f1800be19a04f4d98e2b2f57ff0071e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37fa299c9ef413d8443f7f50a62815f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f8a772441e584d5f952053b0e67af017",
            "value": "â€‡352/352â€‡[00:01&lt;00:00,â€‡222.44â€‡examples/s]"
          }
        },
        "737bc35bae3343239ab2af372ac0b414": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ae0e304507472cb701570c4091f9f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69109c725cbf495e92b2c81b37614921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef669941fd294fadafc55f978b25fc38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8676bbb977f44078ac1a9fd9f8b9850c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f37fa299c9ef413d8443f7f50a62815f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a772441e584d5f952053b0e67af017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wesberg05/Mental-Health-AI-Model/blob/main/MentalhealthAIModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pj3v-c1hS_yt",
        "outputId": "4a526297-387c-4564-8e92-e00fb79e5d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "# Define the model name and load the tokenizer and model\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g765EVJcTT8h",
        "outputId": "c6eed8bf-d8cd-432e-ced3-ccaaa0f49f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Print out which device we're using (GPU or CPU)\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHwp4tpgTkMP",
        "outputId": "16b3d687-44d1-4997-a496-d4df7dce4bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
        "\n",
        "# Split the dataset into training and evaluation sets\n",
        "dataset_split = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = dataset_split['train']\n",
        "eval_dataset = dataset_split['test']"
      ],
      "metadata": {
        "id": "8YBmGTmMV59c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the column names of the dataset\n",
        "print(dataset.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtuDXgdEWLvY",
        "outputId": "4ccaab69-8822-413e-8b53-7f2986930e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': ['Context', 'Response']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(examples):\n",
        "    # Use 'Context' as the input and 'Response' as the target\n",
        "    inputs = [doc for doc in examples['Context']]\n",
        "    targets = [doc for doc in examples['Response']]\n",
        "\n",
        "    # Tokenize the input conversations (context)\n",
        "    model_inputs = tokenizer(inputs, max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Tokenize the responses (targets)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Attach the tokenized responses as labels\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Move the tokenized inputs and labels to the appropriate device (GPU/CPU)\n",
        "    model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the training and evaluation datasets\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "bc040ad0865b485e96b2f737ec9de857",
            "a32420b6eaab418c901de07c3973794e",
            "212316f1377f408d9cded47c9882a91c",
            "2f1800be19a04f4d98e2b2f57ff0071e",
            "737bc35bae3343239ab2af372ac0b414",
            "13ae0e304507472cb701570c4091f9f3",
            "69109c725cbf495e92b2c81b37614921",
            "ef669941fd294fadafc55f978b25fc38",
            "8676bbb977f44078ac1a9fd9f8b9850c",
            "f37fa299c9ef413d8443f7f50a62815f",
            "f8a772441e584d5f952053b0e67af017"
          ]
        },
        "id": "Q-q6Hk27V9Hj",
        "outputId": "27d1f19b-096e-4e3e-d434-9383543671cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/352 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc040ad0865b485e96b2f737ec9de857"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4126: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir='./results',              # Directory to save the model checkpoints\n",
        "    evaluation_strategy=\"epoch\",         # Evaluate at the end of every epoch\n",
        "    learning_rate=2e-5,                  # Learning rate for the optimizer\n",
        "    per_device_train_batch_size=8,       # Batch size for training\n",
        "    per_device_eval_batch_size=8,        # Batch size for evaluation\n",
        "    weight_decay=0.01,                   # Add weight decay to prevent overfitting\n",
        "    save_total_limit=3,                  # Only keep the last 3 checkpoints\n",
        "    num_train_epochs=10,                  # Number of epochs (iterations over the dataset)\n",
        "    predict_with_generate=True,          # Enable text generation during evaluation\n",
        "    logging_dir=\"./logs\",                # Directory to store training logs\n",
        "    logging_steps=100,                   # Log every 100 steps\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ3rocooWZ5v",
        "outputId": "847538ce-907d-4395-d607-6824f2d9910b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,                           # The pre-trained model (FLAN-T5)\n",
        "    args=training_args,                    # Training arguments defined above\n",
        "    train_dataset=tokenized_train_dataset, # Tokenized training dataset\n",
        "    eval_dataset=tokenized_eval_dataset,   # Tokenized evaluation dataset\n",
        "    tokenizer=tokenizer                    # Tokenizer used for tokenization\n",
        ")"
      ],
      "metadata": {
        "id": "29xnE6kZWdP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start the training process\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "MY4wD6AEWd4x",
        "outputId": "0cad3d0c-2c47-4885-b814-21527a9b9fc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3950' max='3950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3950/3950 28:21, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.917600</td>\n",
              "      <td>3.336186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.434300</td>\n",
              "      <td>2.981055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.172600</td>\n",
              "      <td>2.899305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.103200</td>\n",
              "      <td>2.870415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.076900</td>\n",
              "      <td>2.852989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.056800</td>\n",
              "      <td>2.840460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.007700</td>\n",
              "      <td>2.832532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.073400</td>\n",
              "      <td>2.827158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.037500</td>\n",
              "      <td>2.824168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.017700</td>\n",
              "      <td>2.823113</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3950, training_loss=3.2708146377756626, metrics={'train_runtime': 1701.5038, 'train_samples_per_second': 18.572, 'train_steps_per_second': 2.321, 'total_flos': 5874139948646400.0, 'train_loss': 3.2708146377756626, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(context):\n",
        "    # Tokenize the input context\n",
        "    inputs = tokenizer(context, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "    # Generate a response with more flexibility in length\n",
        "    response_ids = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_length=350,            # Increase max_length for longer responses\n",
        "        min_length=125,             # Set a minimum length to ensure responses are substantive\n",
        "        num_beams=8,               # Increase number of beams for diversity\n",
        "        early_stopping=True,        # Allow early stopping if the response is complete\n",
        "        no_repeat_ngram_size=2,    # Prevent repetition of phrases\n",
        "        temperature=0.9,           # Introduce some randomness to the generation\n",
        "        top_k=50,                  # Limit to the top 50 choices\n",
        "        top_p=0.95                 # Nucleus sampling for diverse output\n",
        "    )\n",
        "\n",
        "    # Decode the generated response and return it\n",
        "    return tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Test the model with a sample input context\n",
        "sample_context = \"\"\"\n",
        "Client: Iâ€™ve been feeling really anxious about my job lately. My workload has increased significantly, and I feel like Iâ€™m always on edge, worrying that I might not meet my deadlines. I also find it hard to concentrate, and sometimes I even feel physically sick when I think about work. I want to perform well, but this pressure is overwhelming. How can I manage this anxiety and find a better work-life balance?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response from the model\n",
        "generated_response = generate_response(sample_context)\n",
        "print(\"Generated Response:\", generated_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIV80XE9dizZ",
        "outputId": "1a6f1063-a4cb-4a19-d0e3-811c0d967a82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: I'm sorry to hear that you've been feeling anxious about your job. It's a great feeling to be able to focus on what you are doing and what is going on in your life. There are many reasons why you feel anxious, but you can't be sure if you have the courage to do so. You're not alone. If you want to work on your own, you may need to have some time to think about how you would like to improve your work-life balance. The best way to manage this anxiety is to try to figure out how to deal with it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "\n",
        "# Publish the model\n",
        "from google.colab import userdata\n",
        "\n",
        "REPO_NAME = \"MentalHealthChatbot\"\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "# save model and tokenizer\n",
        "model.save_pretrained(REPO_NAME)\n",
        "tokenizer.save_pretrained(REPO_NAME)\n",
        "\n",
        "# push model and tokenizer to huggingface\n",
        "model.push_to_hub(REPO_NAME, token=HF_TOKEN)\n",
        "tokenizer.push_to_hub(REPO_NAME, token=HF_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "dpoJEXE118aW",
        "outputId": "479a8b9c-a33b-4ddf-e93f-9299de1bc9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/wesberg05/MentalHealthChatbot/commit/54d03fa08010ef980db76206cf7abe6750643e99', commit_message='Upload tokenizer', commit_description='', oid='54d03fa08010ef980db76206cf7abe6750643e99', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required modules\n",
        "from transformers import pipeline\n",
        "\n",
        "# Check if a GPU is available\n",
        "import torch\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Load the fine-tuned model from Hugging Face Hub\n",
        "model = pipeline(\"text2text-generation\", model=\"wesberg05/MentalHealthChatbot\", device=device)\n",
        "\n",
        "print(\"Environment set up. Model loaded on:\", \"GPU\" if device == 0 else \"CPU\")\n",
        "\n",
        "def generate_response(context):\n",
        "    # Generate a response from the model with specified constraints\n",
        "    response = model(context,\n",
        "                     max_new_tokens=350,       # Adjust as needed for response length\n",
        "                     min_length=125,            # Minimum length of the response\n",
        "                     num_beams=8,              # Number of beams for diversity\n",
        "                     early_stopping=True,       # Stop early if the response is complete\n",
        "                     no_repeat_ngram_size=2,    # Prevent repetition of phrases\n",
        "                     do_sample=True,           # Enable sampling\n",
        "                     temperature=0.9,          # Introduce randomness\n",
        "                     top_k=50,                 # Limit to top 50 choices\n",
        "                     top_p=0.95                # Nucleus sampling for diverse output\n",
        "    )\n",
        "\n",
        "    # Return the generated response text\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "# Test the model with a sample input context\n",
        "sample_context = \"\"\"\n",
        "Client: Iâ€™ve been feeling really anxious about my job lately. My workload has increased significantly, and I feel like Iâ€™m always on edge, worrying that I might not meet my deadlines. I also find it hard to concentrate, and sometimes I even feel physically sick when I think about work. I want to perform well, but this pressure is overwhelming. How can I manage this anxiety and find a better work-life balance?\n",
        "\"\"\"\n",
        "\n",
        "# Generate a response from the model\n",
        "generated_response = generate_response(sample_context)\n",
        "print(\"Generated Response:\", generated_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPuF0QJF3W0-",
        "outputId": "b004be2e-04fa-46fd-a96d-9210f1186cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment set up. Model loaded on: GPU\n",
            "Generated Response: I'm sorry to hear that you've been feeling anxious about your job. It sounds like you have a lot of work to do, so it's important to make sure you are able to manage your anxiety and focus on your work. There are many ways you can manage this anxiety as well. If you don't have time to think about it, it may be helpful to talk to your supervisor about how you feel. You can also talk with your boss about what is going on in your life, and ask if he/she is willing to help you with the anxiety.\n"
          ]
        }
      ]
    }
  ]
}